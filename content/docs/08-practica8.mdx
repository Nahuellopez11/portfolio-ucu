---
title: "Pr√°ctica 8"
date: 2025-10-04
order: 8
---

# Pr√°ctica 8: An√°lisis de importancia de features

- **Autores:** Joaqu√≠n Batista, Milagros Cancela, Valent√≠n Rodr√≠guez, Alexia Aurrecoechea, Nahuel L√≥pez (G1)
- **Fecha:** Octubre 2025
- **Entorno:** Python + Pandas + Scikit-learn + Matplotlib + Seaborn + Numpy
- **Dataset:** Dataset inmobiliario con an√°lisis de importancia de variables

## üéØ Objetivo

El prop√≥sito de esta pr√°ctica es realizar un **an√°lisis completo de importancia de features** utilizando m√∫ltiples metodolog√≠as para identificar las variables m√°s relevantes en un dataset inmobiliario. A trav√©s de t√©cnicas como Mutual Information y Random Forest, exploramos las distribuciones de las variables y su impacto en modelos predictivos.

- **Evaluar** importancia de features usando diferentes metodolog√≠as (Mutual Information vs Random Forest)
- **Analizar** distribuciones de variables transformadas y derivadas
- **Identificar** patrones en las relaciones entre variables
- **Comparar** efectividad de diferentes t√©cnicas de selecci√≥n de variables
- **Visualizar** insights sobre el dataset inmobiliario

---

## üìä Dataset y Variables Analizadas

### Variables Principales

| Variable | Tipo | Descripci√≥n |
|----------|------|-------------|
| bedrooms | Original | N√∫mero de dormitorios |
| sqft | Original | Pies cuadrados |
| sqrt_sqft | Transformada | Ra√≠z cuadrada de pies cuadrados |
| sqft_squared | Transformada | Pies cuadrados al cuadrado |
| sqft_per_bedroom | Derivada | Pies cuadrados por dormitorio |
| year_built | Original | A√±o de construcci√≥n |
| bathrooms | Original | N√∫mero de ba√±os |
| garage_spaces | Original | Espacios de garaje |
| lot_size | Original | Tama√±o del lote |
| distance_to_city | Original | Distancia a la ciudad |

---

## üîç An√°lisis de Importancia de Features

### Comparaci√≥n Mutual Information vs Random Forest

#### Top 10 Features - Mutual Information

![Top 10 Features - Mutual Information](/practico%208/feature-importance-mutual-info.png)

**Hallazgos clave:**
- **bedrooms** domina con ~0.018 de informaci√≥n mutua
- Variables transformadas (sqrt_sqft, sqft_squared) muestran importancia moderada
- Variables espaciales (distance_to_city, lot_size) presentan baja importancia
- year_built y sqft_per_bedroom muestran valores intermedios

#### Top 10 Features - Random Forest

![Top 10 Features - Random Forest](/practico%208/feature-importance-random-forest.png)

**Insights comparativos:**
- **crime_rate** emerge como la variable m√°s importante (0.15)
- **lot_size** y **school_rating** muestran alta relevancia
- **distance_to_city** adquiere mayor importancia que en Mutual Information
- Variables transformadas mantienen importancia consistente

---

## üìà An√°lisis de Distribuciones

### Distribuciones de Variables Transformadas

![Distribuciones de Variables](/practico%208/feature-distributions-analysis.png)

**Caracter√≠sticas observadas:**
- **price_per_sqft**: Distribuci√≥n sesgada a la derecha (pico en 1200-1800)
- **sqft_per_bedroom**: Distribuci√≥n altamente sesgada (pico en 20-50)
- **property_age**: Distribuci√≥n relativamente uniforme
- **log_price**: Distribuci√≥n aproximadamente normal (transformaci√≥n exitosa)
- **sqrt_sqft**: Distribuci√≥n normal con pico en 10.5-12.5
- **sqft_squared**: Distribuci√≥n sesgada con cola larga

### Distribuciones de Variables Derivadas

![Distribuciones de Variables Derivadas](/practico%208/derived-features-distributions.png)

**Variables derivadas analizadas:**
- **space_efficiency**: Distribuci√≥n muy sesgada (pico en 0.01-0.02)
- **crowded_property**: Distribuci√≥n similar a space_efficiency
- **location_score**: Distribuci√≥n aproximadamente normal (pico en 0.5-0.6)

---

## üîß Implementaci√≥n T√©cnica

### Pipeline de An√°lisis

```python
# An√°lisis de Importancia con Mutual Information
from sklearn.feature_selection import mutual_info_regression

mi_scores = mutual_info_regression(X, y)
feature_importance_mi = pd.DataFrame({
    'feature': feature_names,
    'importance': mi_scores
}).sort_values('importance', ascending=False)

# An√°lisis con Random Forest
from sklearn.ensemble import RandomForestRegressor

rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X, y)
feature_importance_rf = pd.DataFrame({
    'feature': feature_names,
    'importance': rf.feature_importances_
}).sort_values('importance', ascending=False)
```

### Visualizaciones Implementadas
- Gr√°ficos de barras horizontales para comparar importancia
- Histogramas para an√°lisis de distribuciones
- Subplots para visualizaci√≥n comparativa
- Paletas de colores consistentes para mejor interpretaci√≥n

---

## üìö Insights y Conclusiones

### 1. Divergencia en Metodolog√≠as
- **Mutual Information** prioriza variables estructurales (bedrooms, transformaciones de sqft)
- **Random Forest** enfatiza variables contextuales (crime_rate, school_rating, lot_size)

### 2. Efectividad de Transformaciones
- Transformaciones logar√≠tmicas (log_price) logran distribuciones normales
- Transformaciones cuadr√°ticas (sqft_squared) pueden introducir sesgo
- Variables derivadas muestran distribuciones interesantes pero sesgadas

### 3. Variables Clave Identificadas
- **Estructurales**: bedrooms, sqft (y sus transformaciones)
- **Contextuales**: crime_rate, school_rating, lot_size
- **Derivadas**: sqft_per_bedroom, space_efficiency

### 4. Recomendaciones
- Combinar metodolog√≠as para selecci√≥n robusta de features
- Aplicar transformaciones para normalizar distribuciones sesgadas
- Considerar variables derivadas como features adicionales
- Validar importancia con diferentes algoritmos

---

## üß† Aprendizajes Adquiridos

### Metodolog√≠as Complementarias
- Mutual Information y Random Forest ofrecen perspectivas diferentes pero valiosas
- La importancia de las transformaciones puede cambiar significativamente la importancia relativa
- Las distribuciones sesgadas requieren atenci√≥n especial en el preprocessing

### Variables Derivadas
- Pueden capturar relaciones complejas no evidentes en variables originales
- Es crucial validar la importancia con diferentes metodolog√≠as
- La combinaci√≥n de enfoques estad√≠sticos y de ML proporciona una visi√≥n m√°s completa

---

## ü§î Preguntas para Reflexionar

### ¬øCu√°les fueron las features m√°s importantes seg√∫n tu an√°lisis?

Las variables m√°s relevantes fueron **price_per_sqft** y **property_age**, ya que mostraron una relaci√≥n clara con el precio de venta. El precio por unidad de superficie realza las diferencias de valor entre propiedades, mientras que la antig√ºedad reflej√≥ la bajada de precio esperada a lo largo del tiempo.

### ¬øQu√© sorpresas encontraste en los datos?

En los datos reales, algunas propiedades presentaban valores que no segu√≠an las tendencias esperadas. Esto resalta la importancia de validar supuestos y considerar el contexto del negocio inmobiliario.

### ¬øC√≥mo podr√≠as mejorar el proceso de feature engineering?

Podr√≠a mejorarse incorporando:
- Transformaciones no lineales (logar√≠tmicas o polinomiales)
- Normalizaci√≥n de escalas
- Creaci√≥n de variables interactivas que combinen aspectos estructurales y de ubicaci√≥n

### ¬øQu√© otras t√©cnicas de feature engineering conoces?

- **Codificaci√≥n categ√≥rica**: One-Hot, Target Encoding, Ordinal
- **Escalado y normalizaci√≥n**: MinMaxScaler, StandardScaler
- **Transformaciones matem√°ticas**: log, ra√≠z cuadrada, Box-Cox
- **Binning y discretizaci√≥n** de variables continuas
- **Generaci√≥n autom√°tica de interacciones** con PolynomialFeatures
- **Selecci√≥n de variables** mediante Lasso, Random Forest, o Recursive Feature Elimination (RFE)

### ¬øQu√© diferencias notaste entre datos sint√©ticos y reales?

Los datos sint√©ticos presentan relaciones m√°s limpias, predecibles y controladas, lo que facilita detectar patrones. En cambio, los datos reales incluyen ruido, valores at√≠picos y correlaciones espurias, lo que hace el an√°lisis m√°s desafiante pero tambi√©n m√°s representativo del comportamiento real del mercado. Adem√°s, los datos reales requieren mayor atenci√≥n en limpieza, imputaci√≥n y validaci√≥n de supuestos.

---

## üõ†Ô∏è Herramientas y Tecnolog√≠as

### Librer√≠as Utilizadas
- **scikit-learn**: Feature selection, Mutual Information, Random Forest
- **pandas** y **numpy**: Manipulaci√≥n y an√°lisis de datos
- **matplotlib** y **seaborn**: Visualizaci√≥n avanzada
- **scipy.stats**: An√°lisis estad√≠stico

### T√©cnicas Implementadas
- **Mutual Information**: Medida de dependencia estad√≠stica
- **Random Forest**: Importancia basada en impureza
- **Transformaciones**: Logar√≠tmicas, cuadr√°ticas, derivadas
- **Visualizaci√≥n**: Histogramas, gr√°ficos de barras, subplots

---

## üîó Recursos y Referencias

- **Scikit-learn Documentation**: [Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html)
- **Seaborn Gallery**: [Statistical Data Visualization](https://seaborn.pydata.org/examples/index.html)
- **Pandas Documentation**: [Data Manipulation](https://pandas.pydata.org/docs/user_guide/index.html)
- **Matplotlib Tutorials**: [Advanced Plotting](https://matplotlib.org/stable/tutorials/index.html)