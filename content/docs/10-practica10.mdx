---
title: "PrÃ¡ctica 10"
date: 2025-10-23
order: 10
---

# PrÃ¡ctica 10: PCA y Feature Selection

- **Autores:** JoaquÃ­n Batista, Milagros Cancela, ValentÃ­n RodrÃ­guez, Alexia Aurrecoechea, Nahuel LÃ³pez (G1)
- **Unidad temÃ¡tica:** UT3 Â· Feature Engineering
- **Tipo:** PrÃ¡ctica guiada â€“ Assignment UT3-10
- **Entorno:** Python Â· Pandas Â· Scikit-learn Â· Matplotlib Â· Seaborn Â· NumPy
- **Dataset:** Ames Housing (2â€¯930 registros, 81 variables)
- **Fecha:** Octubre 2025
- **Notebook:** PrÃ¡ctica 10 - PCA y Feature Selection
- **Tiempo estimado:** 90â€“110 minutos

---

## ğŸ¯ Objetivos de Aprendizaje

- **Implementar** PCA y analizar la varianza explicada por los componentes principales.
- **Aplicar** mÃºltiples enfoques de feature selection (Filter, Wrapper, Embedded).
- **Comparar** PCA vs Feature Selection en un caso de negocio real.
- **Evaluar** trade-offs entre reducciÃ³n dimensional y performance del modelo.

---

## ğŸ“Š Dataset y Contexto de Negocio

### Ames Housing Dataset

- **Registros:** 2â€¯930 casas vendidas en Ames, Iowa (2006-2010).
- **Variables:** 81 features (38 numÃ©ricas, 43 categÃ³ricas).
- **Target:** `SalePrice` (precio de venta en USD).
- **CaracterÃ­sticas clave:**
  - Dimensiones: `LotArea`, `GrLivArea`, `TotalBsmtSF`, `GarageArea`.
  - Calidad: `OverallQual`, `KitchenQual`, `ExterQual`.
  - Temporales: `YearBuilt`, `YearRemodAdd`, `GarageYrBlt`.
  - CategÃ³ricas: `Neighborhood`, `HouseStyle`, `RoofStyle`.

**Contexto de negocio:** Como data scientist inmobiliario, la meta es reducir la complejidad del modelo, explicar quÃ© variables impactan en el precio y evitar overfitting eliminando atributos redundantes.

---

## ğŸ”¬ MetodologÃ­as Implementadas

### 1. PCA (AnÃ¡lisis de Componentes Principales)

#### 1.1 EstandarizaciÃ³n

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

print(f"Mean: {X_scaled.mean():.6f}")
print(f"Std: {X_scaled.std():.6f}")
```

- PCA es sensible a la escala: sin estandarizar, variables como `GrLivArea` dominan por magnitud.

#### 1.2 AnÃ¡lisis de varianza explicada

- PC1 explica 13.4â€¯% de la varianza, PC2 5.0â€¯%, PC3 4.7â€¯%.
- Se necesitan 39 componentes para conservar 80â€¯% de informaciÃ³n y 52 componentes para 90â€¯%.
- ReducciÃ³n dimensional obtenida:
  - Original: 81 features.
  - PCA 80â€¯%: 39 componentes â†’ 51.9â€¯% de reducciÃ³n.
  - PCA 90â€¯%: 52 componentes â†’ 35.8â€¯% de reducciÃ³n.

![Scree plot de varianza explicada](/practico10/ames-pca-scree-plot.png)

#### 1.3 InterpretaciÃ³n de componentes principales

- PC1 captura tamaÃ±o/calidad general (`OverallQual`, `YearBuilt`, `GarageCars`).
- PC2 describe distribuciÃ³n y configuraciÃ³n de ambientes (`TotRmsAbvGrd`, `1stFlrSF`).

![Loadings PC1 vs PC2](/practico10/ames-pca-loadings-plot.png)

#### 1.4 ProyecciÃ³n de datos

- La proyecciÃ³n PC1 vs PC2 muestra un gradiente de precios: casas mÃ¡s caras se concentran en valores altos de PC1.

![ProyecciÃ³n de datos en el espacio PCA](/practico10/ames-pca-projection.png)

#### 1.5 Feature selection basada en loadings

- Se sumaron los loadings absolutos de los primeros 39 componentes para rankear features originales.
- Top variables: `RoofMatl`, `Functional`, `ScreenPorch`, `MoSold`, `Heating`.
- Mantiene interpretabilidad, aunque perdiÃ³ rendimiento comparado con mÃ©todos supervisados.

![Importancia de features derivada de PCA](/practico10/ames-pca-feature-importance.png)

### 2. Filter Methods â€“ SelecciÃ³n estadÃ­stica

#### 2.1 F-test (ANOVA para regresiÃ³n)

- EvalÃºa correlaciÃ³n lineal con el target.
- RMSE: \$26â€¯491 Â± \$4â€¯044 Â· RÂ²: 0.8875 Â± 0.0288.
- Selecciona principalmente variables estructurales (`OverallQual`, `GrLivArea`, `GarageCars`).

![Top features seleccionados por F-test](/practico10/ames-f-test-features.png)

#### 2.2 Mutual Information (MI)

- Captura relaciones lineales y no lineales.
- RMSE: \$26â€¯137 Â± \$4â€¯111 Â· RÂ²: 0.8903 Â± 0.0293 (mejor resultado).
- Coincidencia del 76.9â€¯% con F-test; agrega seÃ±ales adicionales de patrones no lineales.

### 3. Wrapper Methods â€“ SelecciÃ³n basada en modelo

- EvalÃºan subconjuntos entrenando un modelo repetidamente.
- **Forward Selection:** 19 features, 62.5â€¯s, RMSE \$40â€¯768 (sobreajuste).
- **Backward Elimination:** 19 features, 57.3â€¯s, RMSE \$41â€¯788.
- **RFE:** 0.8â€¯s, RMSE \$41â€¯767, resultados alineados con forward/backward.

![Ranking de features con RFE](/practico10/ames-rfe-ranking.png)

### 4. Embedded Methods

- PCA reduce dimensionalidad (80â€“90â€¯%) pero pierde interpretabilidad.
- MÃ©todos con regularizaciÃ³n (p.ej. Lasso) se proponen para nuevos experimentos.

---

## ğŸ“Š ComparaciÃ³n Final de MÃ©todos

| MÃ©todo | RMSE | RÂ² | ReducciÃ³n | Interpretable |
|--------|------|----|-----------|---------------|
| Mutual Information | **\$26â€¯137** | **0.890** | 52â€¯% | âœ… |
| F-test | \$26â€¯491 | 0.888 | 52â€¯% | âœ… |
| PCA Componentes | \$26â€¯715 | 0.885 | 52â€¯% | âŒ |
| Dataset original | \$26â€¯807 | 0.885 | 0â€¯% | âœ… |
| Forward Selection | \$40â€¯768 | 0.736 | 77â€¯% | âœ… |
| RFE | \$41â€¯767 | 0.723 | 77â€¯% | âœ… |
| Backward Elimination | \$41â€¯788 | 0.723 | 77â€¯% | âœ… |

ğŸ† **Ganador absoluto:** Mutual Information (mejor RMSE y RÂ² con interpretabilidad).

---

## ğŸ§ª InvestigaciÃ³n Libre: Incremental PCA

- Ideal para datasets que no caben en memoria.
- Se procesan batches de 1â€¯000 observaciones usando `IncrementalPCA`.
- Las primeras 5 componentes explican ~29.5â€¯% de la varianza total.
- Ventajas: reduce uso de memoria y escala a millones de registros.

![Varianza explicada con Incremental PCA](/practico10/ames-incremental-pca-variance.png)

---

## ğŸ¤” ReflexiÃ³n y Conclusiones

1. **Â¿Todas las 80+ features son igual de importantes?**  
   No. Muchas aportan ruido o redundancia y afectan la generalizaciÃ³n.
2. **Â¿QuÃ© problemas genera tener tantas variables?**  
   Overfitting, mayor costo computacional y dificultades de interpretaciÃ³n.
3. **Â¿Diferencia entre PCA y Feature Selection?**  
   PCA crea componentes no supervisados; Feature Selection retiene variables originales (con o sin supervisiÃ³n).

---

## ğŸ’¡ Insights Clave

- **Mutual Information** superÃ³ a PCA en precisiÃ³n y explicabilidad.
- Los **loadings de PCA** no garantizan buen rendimiento supervisado.
- Los **wrapper methods** fueron lentos y empeoraron las mÃ©tricas por sobre-selecciÃ³n.
- Los **filter methods** ofrecieron el mejor balance entre performance y costo computacional.

---

## ğŸš€ Recomendaciones para ProducciÃ³n

- Usar **Mutual Information** para seleccionar ~39 features interpretables.
- Reservar **PCA** para escenarios donde la interpretabilidad sea secundaria.
- Evaluar **Lasso/Elastic Net** como mÃ©todos embedded adicionales.
- Versionar los subconjuntos de variables y documentar los criterios de selecciÃ³n.

---

## ğŸ“ Datasets Utilizados

- **Ames Housing Dataset** (Kaggle Â· UCI ML Repository)  
  2â€¯930 propiedades, 81 features, target `SalePrice`.

---

## ğŸ”— Recursos y Referencias

- Scikit-learn Documentation â€“ PCA, Feature Selection.
- Kaggle Learn â€“ Feature Engineering.
- ArtÃ­culo: *â€œPCA vs Feature Selectionâ€* (Towards Data Science).
