---
title: "Pr√°ctica 16"
date: 2025-12-17
order: 16
description: "Cloud Dataprep - Pipeline ETL Visual"
---

# Pr√°ctica 16: Cloud Dataprep - Pipeline ETL Visual

- **Autores:** Nahuel L√≥pez (G1)
- **Unidad tem√°tica:** Cloud Computing ¬∑ Google Cloud Platform
- **Tipo:** Pr√°ctica guiada ‚Äì Hands-on Lab
- **Entorno:** Cloud Dataprep by Alteryx ¬∑ BigQuery ¬∑ Dataflow
- **Plataforma:** Google Cloud Platform (GCP)
- **Fecha:** Diciembre 2025

---

## üéØ Objetivos de Aprendizaje

- **Conectar** datasets de BigQuery a Cloud Dataprep para su procesamiento.
- **Explorar** la calidad de datos utilizando las herramientas visuales de Dataprep.
- **Construir** un pipeline de transformaci√≥n de datos mediante interfaz visual.
- **Aplicar** t√©cnicas de limpieza y enriquecimiento de datos sin c√≥digo.
- **Ejecutar** jobs de transformaci√≥n y exportar resultados a BigQuery.

---

## üìä Contexto del Laboratorio

En esta pr√°ctica se utiliza **Cloud Dataprep by Alteryx**, un servicio de preparaci√≥n de datos que permite explorar, limpiar y transformar datos estructurados y no estructurados mediante una interfaz visual intuitiva.

**Objetivo principal:** Construir un pipeline ETL completo que procese datos crudos de comercio electr√≥nico y los prepare para an√°lisis posterior, todo sin necesidad de escribir c√≥digo complejo.

**Dataset utilizado:** Datos de sesiones de e-commerce (`all_sessions_raw_dataprep`) almacenados en BigQuery, que contienen informaci√≥n sobre visitas, productos, transacciones y comportamiento de usuarios.

---

## üîß Metodolog√≠as Aplicadas

### Parte A ‚Äî Configuraci√≥n Inicial y BigQuery

#### Acceso a Cloud Dataprep

El primer paso consisti√≥ en acceder a Cloud Dataprep desde la consola de Google Cloud Platform:

1. **Navegaci√≥n:** Se accedi√≥ a Cloud Dataprep desde el men√∫ de navegaci√≥n de la consola.
2. **T√©rminos de servicio:** Se aceptaron los t√©rminos de servicio necesarios para utilizar el servicio.
3. **Habilitaci√≥n:** Se verific√≥ que las APIs requeridas estuvieran habilitadas en el proyecto.

#### Preparaci√≥n del Dataset en BigQuery

Aunque el foco principal es Dataprep, BigQuery act√∫a como punto de entrada y salida de datos:

1. **Creaci√≥n de dataset:** Se cre√≥ un dataset llamado `ecommerce` en BigQuery para almacenar los datos de trabajo.
2. **Copia de datos:** Se ejecut√≥ una consulta SQL para copiar un subconjunto de datos p√∫blicos (`all_sessions_raw_dataprep`) al dataset creado.
3. **Verificaci√≥n:** Se confirm√≥ que los datos se copiaron correctamente y est√°n disponibles para su procesamiento.

**Resultado:** Dataset preparado en BigQuery listo para ser importado a Dataprep.

---

### Parte B ‚Äî Conexi√≥n de Datos a Dataprep

#### Creaci√≥n del Flow

Un "Flow" en Dataprep es un contenedor que agrupa todas las transformaciones de un pipeline:

1. **Nuevo Flow:** Se cre√≥ un nuevo Flow llamado "Ecommerce Analytics Pipeline".
2. **Importaci√≥n:** Se import√≥ el dataset `all_sessions_raw_dataprep` desde BigQuery al Flow.
3. **Conexi√≥n:** Se a√±adi√≥ el dataset al flujo de trabajo, permitiendo que Dataprep acceda a los datos.

**Ventaja:** La conexi√≥n directa con BigQuery permite trabajar con grandes vol√∫menes de datos sin necesidad de descargarlos localmente.

---

### Parte C ‚Äî Exploraci√≥n de Datos

#### Vista Transformer

Dataprep carga autom√°ticamente una muestra representativa del dataset en la vista "Transformer", donde se pueden visualizar y analizar los datos:

**An√°lisis de calidad realizado:**

1. **Valores nulos:**
   - Se identificaron columnas con alto porcentaje de valores faltantes.
   - Ejemplos: `itemQuantity` y `itemRevenue` conten√≠an √∫nicamente valores nulos.

2. **Tipos de datos incorrectos:**
   - Se detect√≥ que `productSKU` fue inferido como entero cuando deber√≠a ser string.
   - Esto puede causar problemas en transformaciones posteriores.

3. **Distribuciones:**
   - Se analiz√≥ la distribuci√≥n de valores en columnas num√©ricas.
   - Ejemplo: `sessionQualityDim` mostr√≥ un sesgo hacia valores bajos.

4. **Valores at√≠picos:**
   - Se identificaron outliers y patrones inesperados en los datos.
   - Se detectaron inconsistencias que requieren limpieza.

**Insight:** La exploraci√≥n visual permite identificar problemas de calidad de datos de manera r√°pida y eficiente, guiando las decisiones de limpieza.

---

### Parte D ‚Äî Limpieza de Datos

#### Construcci√≥n de la Recipe

Una "Recipe" en Dataprep contiene todas las transformaciones que se aplicar√°n a los datos. Se implementaron los siguientes pasos de limpieza:

#### 1. Conversi√≥n de Tipos de Datos

**Problema:** La columna `productSKU` fue detectada incorrectamente como entero.

**Soluci√≥n:** Se aplic√≥ una transformaci√≥n para convertir `productSKU` a tipo String, preservando todos los valores y permitiendo manejar c√≥digos alfanum√©ricos.

#### 2. Eliminaci√≥n de Columnas

**Problema:** Las columnas `itemQuantity` e `itemRevenue` conten√≠an √∫nicamente valores nulos, no aportando informaci√≥n √∫til.

**Soluci√≥n:** Se eliminaron estas columnas del dataset para reducir el tama√±o y simplificar el an√°lisis posterior.

#### 3. Deduplicaci√≥n

**Problema:** Exist√≠an filas duplicadas que pod√≠an sesgar los an√°lisis.

**Soluci√≥n:** Se aplic√≥ una transformaci√≥n de deduplicaci√≥n basada en todas las columnas, eliminando registros exactamente iguales y manteniendo solo una instancia de cada fila √∫nica.

#### 4. Filtrado de Datos

Se aplicaron dos filtros principales:

**Filtro 1: Sesiones con ingresos**
- **Condici√≥n:** `totalTransactionRevenue IS NOT NULL`
- **Objetivo:** Mantener solo sesiones que generaron ingresos para el an√°lisis de revenue.

**Filtro 2: Tipo de registro**
- **Condici√≥n:** `type = 'PAGE'`
- **Objetivo:** Mantener √∫nicamente registros de tipo PAGE (vistas de p√°gina) para evitar conteos dobles de eventos.

**Resultado:** Dataset limpio con datos consistentes y sin duplicados.

---

### Parte E ‚Äî Enriquecimiento de Datos

#### Transformaciones Avanzadas

Se implementaron transformaciones para mejorar la calidad y utilidad del dataset:

#### 1. ID √önico de Sesi√≥n

**Problema:** La columna `visitId` no es √∫nica entre diferentes usuarios, lo que puede causar ambig√ºedad.

**Soluci√≥n:** Se cre√≥ una nueva columna `unique_session_id` concatenando `fullVisitorId` y `visitId`:

```
unique_session_id = CONCAT(fullVisitorId, '_', visitId)
```

Esto garantiza un identificador √∫nico para cada sesi√≥n de usuario.

#### 2. Mapeo de Acciones de E-commerce

**Problema:** La columna `eCommerceAction_type` utiliza c√≥digos num√©ricos que no son intuitivos (ej: 0, 1, 2, 6).

**Soluci√≥n:** Se cre√≥ una columna calculada `eCommerceAction_label` utilizando una expresi√≥n CASE para mapear c√≥digos a descripciones legibles:

- `0` ‚Üí 'Unknown'
- `1` ‚Üí 'Click through of product lists'
- `2` ‚Üí 'Product detail views'
- `6` ‚Üí 'Completed purchase'
- etc.

Esto mejora significativamente la legibilidad del dataset para an√°lisis posteriores.

#### 3. Ajuste de Ingresos

**Problema:** La columna `totalTransactionRevenue` almacena valores multiplicados por 10^6, lo que dificulta la interpretaci√≥n directa.

**Soluci√≥n:** Se cre√≥ una nueva columna `totalTransactionRevenue1` dividiendo el valor original por 1,000,000:

```
totalTransactionRevenue1 = totalTransactionRevenue / 1000000
```

Esto proporciona valores de ingresos en la escala correcta para an√°lisis y reportes.

**Resultado:** Dataset enriquecido con columnas derivadas que facilitan el an√°lisis posterior.

---

### Parte F ‚Äî Ejecuci√≥n del Job a BigQuery

#### Configuraci√≥n de Salida

Una vez completadas todas las transformaciones, se configur√≥ la salida del pipeline:

1. **Destino:** Se configur√≥ BigQuery como destino de salida.
2. **Tabla destino:** Se especific√≥ la creaci√≥n de una nueva tabla llamada `revenue_reporting`.
3. **Dataset:** Se seleccion√≥ el dataset `ecommerce` como ubicaci√≥n de la tabla.

#### Ejecuci√≥n del Job

1. **Entorno:** Se ejecut√≥ el job utilizando el entorno Dataflow + BigQuery.
2. **Procesamiento:** Dataflow proces√≥ el dataset completo aplicando todas las transformaciones definidas en la Recipe.
3. **Monitoreo:** Se monitore√≥ el progreso del job desde la interfaz de Dataprep.

#### Verificaci√≥n

Una vez finalizado el job:

1. **BigQuery:** Se accedi√≥ a BigQuery para verificar la creaci√≥n de la tabla.
2. **Validaci√≥n:** Se confirm√≥ que la tabla `revenue_reporting` se cre√≥ correctamente.
3. **Datos:** Se revis√≥ una muestra de los datos transformados para validar que las transformaciones se aplicaron correctamente.

**Resultado:** Tabla `revenue_reporting` en BigQuery con datos limpios, transformados y listos para an√°lisis.

---

## üß† Conclusiones y Aprendizajes

### Resumen de Hallazgos

1. **Pipeline ETL visual:** Se construy√≥ exitosamente un pipeline ETL completo utilizando √∫nicamente una interfaz visual, demostrando que no es necesario escribir c√≥digo complejo para realizar transformaciones de datos.

2. **Exploraci√≥n de calidad:** Cloud Dataprep facilit√≥ la identificaci√≥n de problemas de calidad de datos mediante visualizaciones intuitivas y estad√≠sticas autom√°ticas.

3. **Transformaciones eficientes:** Se aplicaron m√∫ltiples transformaciones (limpieza, enriquecimiento, filtrado) de manera secuencial y organizada mediante la Recipe.

4. **Integraci√≥n con BigQuery:** La integraci√≥n nativa entre Dataprep y BigQuery permite un flujo de trabajo fluido desde la preparaci√≥n hasta el almacenamiento de datos procesados.

5. **Dataset final:** Se obtuvo un dataset limpio y enriquecido (`revenue_reporting`) listo para an√°lisis avanzados, reportes y visualizaciones.

### Ventajas de Cloud Dataprep

- **Sin c√≥digo:** Permite realizar transformaciones complejas sin escribir c√≥digo.
- **Visualizaci√≥n:** Facilita la comprensi√≥n del flujo de transformaciones.
- **Escalabilidad:** Procesa grandes vol√∫menes de datos utilizando Dataflow.
- **Integraci√≥n:** Se integra nativamente con otros servicios de GCP.

### Desaf√≠os Encontrados

- **Inferencia de tipos:** A veces los tipos de datos se infieren incorrectamente y requieren correcci√≥n manual.
- **Costo:** El procesamiento en Dataflow puede ser costoso para datasets muy grandes.
- **Curva de aprendizaje:** Aunque es visual, requiere familiarizarse con las transformaciones disponibles.

---

## üöÄ Pr√≥ximos Pasos

### Automatizaci√≥n

- **Cloud Scheduler:** Explorar la automatizaci√≥n del pipeline mediante Cloud Scheduler para ejecuciones peri√≥dicas (diarias, semanales).
- **Triggers:** Configurar triggers basados en eventos para ejecutar el pipeline cuando nuevos datos lleguen a BigQuery.

### Funcionalidades Avanzadas

- **Expresiones regulares:** Investigar el uso de expresiones regulares personalizadas en Dataprep para limpieza m√°s sofisticada.
- **Transformaciones complejas:** Explorar funciones avanzadas de agregaci√≥n y c√°lculo de columnas.

### Visualizaci√≥n y An√°lisis

- **Looker Studio:** Conectar el dataset resultante en BigQuery con Looker Studio para crear dashboards de monitoreo de ingresos.
- **An√°lisis exploratorio:** Realizar an√°lisis adicionales sobre el dataset limpio para obtener insights de negocio.

### Optimizaci√≥n

- **Costo:** Evaluar el costo del procesamiento en Dataflow para optimizar el uso de recursos en datasets de mayor volumen.
- **Performance:** Analizar el tiempo de ejecuci√≥n y optimizar transformaciones para mejorar la eficiencia.

---

## ‚úÖ Checklist de Implementaci√≥n

- Acceso a Cloud Dataprep desde la consola de GCP
- Creaci√≥n del dataset `ecommerce` en BigQuery
- Copia de datos p√∫blicos al dataset de trabajo
- Creaci√≥n del Flow "Ecommerce Analytics Pipeline"
- Importaci√≥n del dataset desde BigQuery a Dataprep
- Exploraci√≥n visual de calidad de datos en Transformer
- Identificaci√≥n de columnas con valores nulos
- Detecci√≥n de tipos de datos incorrectos
- An√°lisis de distribuciones y valores at√≠picos
- Conversi√≥n de tipos de datos (productSKU a String)
- Eliminaci√≥n de columnas innecesarias (itemQuantity, itemRevenue)
- Deduplicaci√≥n de filas
- Filtrado de sesiones sin ingresos
- Filtrado por tipo de registro (PAGE)
- Creaci√≥n de ID √∫nico de sesi√≥n (unique_session_id)
- Mapeo de c√≥digos de acci√≥n a etiquetas legibles
- Ajuste de escala de ingresos (divisi√≥n por 1,000,000)
- Configuraci√≥n de salida a BigQuery
- Ejecuci√≥n del job en Dataflow
- Verificaci√≥n de la tabla revenue_reporting en BigQuery

---

## üìö Referencias y Recursos

- [Cloud Dataprep Documentation](https://cloud.google.com/dataprep/docs)
- [BigQuery Documentation](https://cloud.google.com/bigquery/docs)
- [Dataflow Documentation](https://cloud.google.com/dataflow/docs)
- [Cloud Dataprep User Guide](https://help.trifacta.com/cloud-dataprep/)
- [Google Cloud ETL Best Practices](https://cloud.google.com/architecture/etl-patterns)

---

*Esta pr√°ctica demuestra c√≥mo Cloud Dataprep simplifica la preparaci√≥n de datos mediante una interfaz visual, permitiendo construir pipelines ETL complejos sin necesidad de programaci√≥n, y facilitando la integraci√≥n con otros servicios de Google Cloud Platform.*
