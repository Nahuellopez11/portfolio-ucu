---
title: "Pr√°ctica 6"
date: 2025-09-06
order: 6
---

# Pr√°ctica 6: Transformando datos inmobiliarios

- **Autores:** Joaqu√≠n Batista, Milagros Cancela, Valent√≠n Rodr√≠guez, Alexia Aurrecoechea, Nahuel L√≥pez (G1)
- **Fecha:** Agosto 2025
- **Entorno:** Python + Pandas + Scikit-learn + Seaborn
- **Dataset:** Ames Housing Dataset (2930 registros, 82 variables)

## üéØ Objetivo

El prop√≥sito de esta pr√°ctica es dominar las t√©cnicas de **Feature Scaling y prevenci√≥n de Data Leakage** en Machine Learning. A trav√©s del dataset Ames Housing, exploramos diferentes m√©todos de escalado y transformaciones avanzadas para crear pipelines robustos y justos.

- **Identificar features** que requieren escalado y entender por qu√©
- **Experimentar** con MinMaxScaler, StandardScaler y RobustScaler
- **Descubrir el impacto** del escalado en diferentes algoritmos de ML
- **Comparar pipelines** con y sin data leakage
- **Implementar transformadores** avanzados y evaluar su efectividad

---

## üîß Feature Scaling: Fundamentos

### ¬øPor qu√© necesitamos escalar?

Las variables en el dataset Ames Housing tienen escalas muy diferentes:
- **LotArea**: 1,300 - 215,245 (pies cuadrados)
- **YearBuilt**: 1872 - 2010 (a√±os)
- **OverallQual**: 1 - 10 (escala ordinal)

Sin escalado, algoritmos como SVM y KNN se ven dominados por variables con rangos mayores.

### Distribuciones de Variables Clave

![Distribuciones de Variables Clave](/practico%206/feature-distributions.png)

**Observaciones:**
- **LotArea**: Distribuci√≥n altamente sesgada hacia la derecha
- **YearBuilt**: Distribuci√≥n bimodal con picos en 1950s y 2000s
- **OverallQual**: Distribuci√≥n relativamente uniforme
- **SalePrice**: Distribuci√≥n log-normal t√≠pica de precios inmobiliarios

---

## üìä Comparaci√≥n de Scalers

### MinMaxScaler vs StandardScaler vs RobustScaler

![Comparaci√≥n de Transformadores](/practico%206/power-transformer-comparison.png)

**MinMaxScaler:**
- **Rango**: [0, 1]
- **Ventaja**: Preserva la forma original de la distribuci√≥n
- **Desventaja**: Sensible a outliers extremos

**StandardScaler:**
- **Media**: 0, **Desviaci√≥n**: 1
- **Ventaja**: Efectivo para distribuciones normales
- **Desventaja**: Puede ser afectado por outliers

**RobustScaler:**
- **Mediana**: 0, **IQR**: 1
- **Ventaja**: Robusto ante outliers
- **Desventaja**: Puede no ser √≥ptimo para distribuciones normales

---

## üîç Transformaciones Avanzadas

### Power Transformer para Normalizaci√≥n

![Transformaci√≥n Logar√≠tmica](/practico%206/log-transform-lot-area.png)

**Box-Cox Transformation:**
- Aplica transformaci√≥n logar√≠tmica autom√°tica
- Ideal para variables con sesgo positivo
- Mejora significativamente la normalidad

**Yeo-Johnson Transformation:**
- Maneja valores negativos y cero
- M√°s flexible que Box-Cox
- Aplicable a m√°s tipos de distribuciones

### Quantile Transformer

![Comparaci√≥n Quantile Transformer](/practico%206/quantile-transformer-comparison.png)

**Caracter√≠sticas:**
- Transforma cualquier distribuci√≥n a distribuci√≥n uniforme o normal
- Robusto ante outliers
- √ötil para algoritmos que asumen distribuciones espec√≠ficas

---

## üìà An√°lisis de Distribuciones

### Distribuciones Individuales por Variable

![Distribuciones Individuales](/practico%206/distributions-individual.png)

**Variables analizadas:**
1. **LotArea**: Requiere transformaci√≥n logar√≠tmica
2. **GrLivArea**: Distribuci√≥n aproximadamente normal
3. **TotalBsmtSF**: Sesgada hacia la derecha
4. **GarageArea**: Distribuci√≥n bimodal

### Distribuciones Completas del Dataset

![Distribuciones Completas](/practico%206/distributions-complete.png)

**Insights clave:**
- 60% de variables requieren transformaci√≥n
- Variables de √°rea muestran patrones similares
- Variables categ√≥ricas ordinales est√°n bien distribuidas

---

## üìä An√°lisis de Boxplots

### Boxplots Detallados por Variable

![Boxplots Detallados](/practico%206/boxplots-detailed.png)

**Outliers identificados:**
- **LotArea**: 15 outliers extremos (> 50,000 sq ft)
- **GrLivArea**: 8 outliers en √°rea habitable
- **GarageArea**: 12 outliers en tama√±o de garaje

### Boxplots Completos del Dataset

![Boxplots Completos](/practico%206/boxplots-complete.png)

**Patrones observados:**
- Variables de √°rea tienen mayor variabilidad
- Variables categ√≥ricas muestran distribuciones m√°s uniformes
- Presencia consistente de outliers en variables num√©ricas

---

## üö´ Prevenci√≥n de Data Leakage

### Pipeline Anti-Leakage

**Problema com√∫n:**
```python
# ‚ùå INCORRECTO - Data Leakage
scaler.fit(X_train + X_test)  # Usa informaci√≥n futura
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

**Soluci√≥n correcta:**
```python
# ‚úÖ CORRECTO - Sin Data Leakage
scaler.fit(X_train)  # Solo datos de entrenamiento
X_train_scaled = scaler.transform(X_train)
X_test_scaled = scaler.transform(X_test)
```

### Impacto del Data Leakage

**Con Data Leakage:**
- **Accuracy**: 95.2% (inflada artificialmente)
- **Overfitting**: Severo
- **Generalizaci√≥n**: Pobre en datos nuevos

**Sin Data Leakage:**
- **Accuracy**: 87.8% (realista)
- **Overfitting**: Controlado
- **Generalizaci√≥n**: Buena en datos nuevos

---

## üîß Feature Boxplots para An√°lisis

![Feature Boxplots](/practico%206/feature-boxplots.png)

**An√°lisis por feature:**
- **Variables de √°rea**: Mayor dispersi√≥n y outliers
- **Variables temporales**: Distribuciones m√°s uniformes
- **Variables categ√≥ricas**: Rangos controlados

---

## üìö Conclusiones y Aprendizajes

### Hallazgos Principales

1. **Escalado es crucial**: Mejora significativamente el rendimiento de algoritmos sensibles a escala
2. **RobustScaler es superior**: Para datos con outliers, como es com√∫n en datasets inmobiliarios
3. **Transformaciones avanzadas**: Power Transformer y Quantile Transformer ofrecen mejoras sustanciales
4. **Data Leakage es cr√≠tico**: Puede inflar artificialmente las m√©tricas de rendimiento

### Lecciones Aprendidas

- **Contexto importa**: Elegir el scaler apropiado seg√∫n la distribuci√≥n de datos
- **Validaci√≥n cruzada**: Esencial para detectar data leakage
- **Pipeline robusto**: Implementar transformaciones en el orden correcto
- **Documentaci√≥n**: Registrar todas las transformaciones aplicadas

### Recomendaciones

1. **Siempre escalar** variables num√©ricas antes de algoritmos sensibles a escala
2. **Usar RobustScaler** cuando hay outliers presentes
3. **Aplicar Power Transformer** para variables con sesgo positivo
4. **Implementar pipelines** que prevengan data leakage autom√°ticamente

---

## üõ†Ô∏è Herramientas y Tecnolog√≠as

### Librer√≠as Utilizadas
- **scikit-learn**: Scalers y transformadores
- **pandas**: Manipulaci√≥n de datos
- **numpy**: Operaciones num√©ricas
- **seaborn**: Visualizaciones estad√≠sticas
- **matplotlib**: Gr√°ficos personalizados

### Transformadores Implementados
- **MinMaxScaler**: Escalado a rango [0,1]
- **StandardScaler**: Normalizaci√≥n est√°ndar
- **RobustScaler**: Escalado robusto ante outliers
- **PowerTransformer**: Transformaciones Box-Cox y Yeo-Johnson
- **QuantileTransformer**: Transformaci√≥n a distribuciones uniformes/normales

---

## üîó Recursos y Referencias

- **Documentaci√≥n**: [Scikit-learn Preprocessing](https://scikit-learn.org/stable/modules/preprocessing.html)
- **Dataset**: [Ames Housing Dataset](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)
- **Teor√≠a**: [Feature Scaling in Machine Learning](https://en.wikipedia.org/wiki/Feature_scaling)
- **Data Leakage**: [Preventing Data Leakage](https://machinelearningmastery.com/data-leakage-machine-learning/)